| Title of Paper | Survey or Article? | Summary of problem addressed | Key Methodologies used | Main findings and contributions | Limitations and open questions | Relevance to chosen topic |
|----------------|---------------------|------------------------------|-------------------------|---------------------------------|--------------------------------|----------------------------|
| A Survey of Federated Learning for Connected and Automated Vehicles | Survey | The article addresses the problem of CAVs(Connected and Automated Vehicles) producing significant amounts of data.<br>In centralized learning (CL), this data must be sent to a central server, which causes privacy risks, high network use, and delays.<br>CAVs are now able to use an ML model called FL(Federated Learning). Edge devices/clients train locally and only send gradients or model parameters to the cloud.<br>This protects local data privacy and makes the system less dependent on network connection. | FL uses the FedAvg algorithm for CAV tasks, specifically for applications like:<br>- In-vehicle human monitoring (driver state detection)<br>- Steering wheel angle prediction (for better control and safety)<br>- Vehicle trajectory prediction (predicting movement paths in traffic)<br>- Object detection (from cameras and LiDAR)<br>- Motion control applications (decision-making for acceleration and braking) | Main findings include how FL works well for connected and Automated Vehicles as it protects data privacy, lowers network costs and allows vehicles to collaborate in training models.<br>FL is being tested in object detection, trajectory prediction, steering angle estimation and motion control, all contributing to CAV tasks. | Data Modality: CAVs collect many kinds of data — combining these very different types is complex.<br>Data Security: Secure aggregation and differential privacy are used, but privacy threats and attacks can still occur.<br>Data Heterogeneity: Non-IID data causes client drift and instability.<br>Lack of Real-world tests: More testing is needed in various conditions.<br>Catastrophic Forgetting remains unresolved.<br>Security and privacy will always need improvement, as issues with grid data are prevalent. | This survey mainly speaks on FL and CAVs but still relates to my topic of LLM quantization for small devices.<br>Just as FL’s primary focus is on privacy, communication, and accuracy, similarly, quantization focuses on accuracy, speed and memory use.<br>The survey gives a base of how to organize methods and challenges, which can be used as a guide for writing my own review on quantization. |
| Federated Continual Learning: Concepts, Challenges, and Solutions | Survey | The article addresses the problem of FL(Federated Learning) and CL(Continual Learning) when they are used together.<br>Using FL on its own doesn’t factor for handling data that changes over time and using CL on its own doesn’t allow for the training of models while protecting privacy.<br>FCL(Federated Continual Learning) combines both FL and CL to allow devices to learn from new data as a whole, collaborate and ensure more safety. | The paper discusses challenges at the intersection of FL and CL, such as:<br>- Catastrophic forgetting<br>- Concept drift<br>- Data heterogeneity<br>- Model stability over time<br>- Communication limits<br><br>Solutions include:<br>- Regularization methods<br>- Replay methods<br>- Dynamic architectures<br>- Knowledge distillation | FCL combines the strengths of FL and CL, allowing distributed clients to handle evolving data while protecting privacy. | **Heterogeneity:** Conceptual, hardware, and software differences make training unstable.<br>**Communication Cost:** Frequent updates across clients create high overhead.<br>Negative Knowledge Transfer: Divergent data across clients can harm performance, making convergence difficult. | Its relevance to quantizing LLMs for low compute devices comes from the fact that this article speaks on how ML can work on low-resource devices.<br>Federated Learning focuses on efficiency while handling slow devices, high communication costs and complex data groups.<br>Similarly, quantizing LLMs makes models faster and smaller so they can run on limited hardware. |
| Federated Learning Intersection Vehicle Trajectory Prediction Scheme Within Digital Twin | Research Article | Digital twin traffic systems face challenges because privacy and security rules do not allow sensitive data like vehicle trajectories and semantic information to be stored in one central place.<br>The paper addresses this problem by proposing using a federated learning model with spatio-temporal-semantic attention (FedSTAST) inside the digital twin framework. | - FedSTAST model combines FL with spatio-temporal-semantic attention<br>- Temporal Convolutional Network (TCN) accurately predicts long-term lane-changing trajectories and driving behaviors with shorter computation time | Main findings include how FedSTAST is more accurate in predicting vehicle trajectories with spatio-temporal-semantic attention than without. | Tests were only simulations, not real-world scenarios, so FedSTAST’s performance is uncertain.<br>Many limitations are reduced since FL and DT keep raw data localized while sharing only model parameters.<br>Remaining challenge: computational cost of spatio-temporal semantic attention with heavy data. | This paper connects to my topic because it’s about keeping models accurate while working with limited resources and privacy rules.<br>Similarly, quantization makes LLMs run on smaller devices by trading off accuracy and efficiency. |
| Towards Explainable Traffic Flow Prediction with Large Language Models | Research Article | The article addresses the problem of traffic prediction models being difficult to understand despite their accuracy.<br>They utilize LLMs to make the language prompts for traffic data more explainable. | - Traffic data (weather, holidays, speed) is converted into text prompts<br>- LoRA is used to fine tune a large language model, which is less costly than retraining the entire model | TF-LLM not only predicts traffic flow reliably but also explains its reasoning in plain language.<br>Instead of just numbers, it links causes like rush hour or weather.<br>A standout feature is zero-shot ability — transferring to a new city without retraining. | The TF-LLM model depends on large LLMs, which are expensive and heavy for low-resource systems.<br>Explanation quality varies depending on prompt design and training examples. | This article connects to my topic because it shows how LLMs can be adapted for new tasks but still face problems with efficiency and resource demands.<br>TF-LLM needs high compute to work well, while quantization shrinks LLMs to run on smaller devices.<br>Both balance accuracy, efficiency, and practicality. |
| Evaluating Quantized Large Language Models for Code Generation on Low-Resource Language Benchmarks | Research Paper | Large Language Models (LLMs) are powerful but hard to run on normal devices as they require a lot of compute.<br>This makes them less accessible to everyday users.<br>Quantization is a way to shrink LLMs so they can run on regular laptops without GPUs. | Tests of five different LLMs were done with post-training quantization:<br>- 2-bit<br>- 4-bit<br>- 8-bit<br><br>Benchmarks: Lua and Python | 4-bit quantization is ideally the best.<br>Models run smoothly on an average CPU-only laptop and still achieve reasonable accuracy.<br>2-bit shows major performance drops.<br>8-bit uses more compute without big improvements. | Even quantized models still perform under 50% accuracy on many code tasks, so they are not yet practical replacements for larger LLMs. | This article directly matches my topic on quantizing LLMs for low-compute devices.<br>It shows how quantization makes it possible to run large models on consumer laptops, while also highlighting the trade-offs in accuracy and efficiency. |
